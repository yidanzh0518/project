---
title: "Creating code embedding matrix"
subtitle: "Midterm project for PHS7045"
format: pdf
editor: visual
author: Yidan Zhang
---

# Introduction

Word embedding has become a significant topic in natural language processing, especially when working with context data. The key concept behind word embedding is based on the idea that a word can be characterized by "the company it keeps," or its context. Word embeddings capture this contextual information by creating a lower-dimensional space (compared to traditional one-hot encoding) where words that appear in similar contexts are positioned close to one another. This allows for more efficient and effective word representation in models.

One of the most widely recognized methods for generating word embeddings is Word2Vec, developed by Google in 2013. This neural network-based approach is trained on large corpora of text, learning to predict the context of a word based on its surrounding words within a local window. While Word2Vec focuses on local context, another prominent method, GloVe (Global Vectors for Word Representation), expands this concept by incorporating global context. GloVe takes advantage of the overall word-to-word co-occurrence patterns across the entire corpus. By using a precomputed, count-based co-occurrence matrix, GloVe leverages global statistical information about how often words appear together to generate its embedding matrix.

With the increasing digitization of patient information, electronic health records (EHR) have become essential for efficiently tracking medical histories and facilitating the exchange of data between healthcare providers. As structured data within EHR systems, ICD (International Classification of Diseases) and CPT (Current Procedural Terminology) codes have long been used to record diagnoses and procedures, respectively, even before the widespread adoption of EHRs. Embedding these medical codes offers several key benefits. It can enhance disease prediction and diagnosis modeling by allowing models to recognize similarities between related conditions, improving predictive performance. Additionally, ICD code embeddings support patient clustering, enabling algorithms like k-means or DBSCAN to group patients based on diagnostic patterns and improve clustering effectiveness. They also help identify complex comorbidity patterns and hidden phenotypes that traditional methods may miss. Finally, code embedding aids in healthcare data integration, which is especially valuable given the challenges of accessing protected data due to data safety concerns.

The goal of this project is to implement the GloVe algorithm to create a package that generates word embedding matrices from EHR data. The package will be tested on the MIMIC-III clinical dataset to generate ICD-9 code embeddings.

# Solution Plan

-   STEP 1:

    The first step in generating embeddings using the GloVe approach is to create a code co-occurrence matrix, $\mathbf{X}$, where each element of $\mathbf{Xâ€‹_{ij}}$ represents how often word $i$ appears in context of word $j$. The function's arguments will include the user's dataset (`data`), along with parameters to specify which column corresponds to the subject in dataset (`id`), which column represents the time or sequence information for defining the context window (`time`), and the user-defined size of the context window (`window`). EHR data typically consists of large datasets with vast amounts of patient information, and each patient have multiple diagnosis codes. To compute the co-occurrences, the function must iterate through each patient and count the co-occurrence of ICD codes within a user-defined context window,with approximately 13,000 ICD-9 codes, this process can be both time-consuming and computationally expensive. To address these challenges, the project will optimize performance by coercing the data to a `data.table` structure for faster reference handling, and applying vectorization for efficient matrix generation to further improve time efficiency.

-   STEP 2:

    The second step is to use the co-occurrence matrix to create the pointwise mutual information (PMI) matrix, which measures the association between a word and a context word. PMI is calculated as $\text{PMI}(w,c) = \frac{p(w, c)}{p(w) \cdot p(c)}$ , where $p(w,c)$ is the count of how often word $w$ and context code $c$ occur in the same context window, divided by the total count of code pairs within the window; and $p(w)$ and $p(c)$ are singleton frequency of the word $w$ and $c$, respectively, within the context window. In machine learning literature, the shifted positive pointwise mutual information (SPPMI) matrix is commonly used, where PMI is shifted by a user-defined constant $log(k)$. To accommodate user-specific needs, another function in the package will be aim to calculate SPPMI, which is defined as $\text{SPPMI}(w,c)=max(\text{PMI}(w,c)-log(k),0)$. The computation starts by obtaining the joint and singleton frequencies for the codes using the function `get_SG`, where vectorization is applied. The two functions will then be used to compute both PMI and SPPMI, respectively.

-   STEP 3:

    In this step, truncated singular value decomposition (SVD) is applied to factorize the PMI/SPPMI matrix and generate lower-dimensional embeddings for the codes. The function allows the user to specify the number of top $p$-dimensional singular values and vectors to retain, as well as the maximum number of iterations for the SVD process. Once executed, the function will return the code embedding matrix $\mathbf{W}= u \cdot \sqrt{d}$ , where the matrix has a dimensionality reduced to the user-defined vector length $p$. To optimize the code's performance, vectorization techniques and the use of `data.table` will be incorporated to accelerate the process and ensure efficient computation.

-   STEP 4 (Time Permitting):

    One approach to help users determine the optimal embedding vector length is to evaluate the performance of the embedding matrix by predicting phecode-based classification accuracy. Phecodes group ICD codes into clinically meaningful categories, and an ideal embedding matrix would accurately capture diagnostic information, allowing for precise classification aligned with phecodes. By assessing the AUC (Area Under the Curve) for different vector lengths, users can identify the length that maximizes performance, minimizing information loss and avoiding overfitting.

# Preliminary Result

-   Data

    The data used to test the package is sourced from the [MIMIC-III clinical dataset](https://physionet.org/content/mimiciii/1.4/), a freely available database containing de-identified health-related data from patients who were admitted to the ICU at Beth Israel Deaconess Medical Center between 2001 and 2012. The dataset includes diagnoses recorded using the 9th version of the ICD coding system, specifically in a table named "DIAGNOSES_ICD." A brief summary of this data is provided below:

    ![Summary of the table](images/clipboard-684336803.png){width="354"}

    The data used for this report is available in the repository and consists of a random sample of 1,000 subjects, resulting in a total of 13,623 observations from the original dataset.

    ```{r}
    #| message: false
    #| include: false
    library(data.table)
    library(tidyverse)
    library(microbenchmark)
    ```

-   Function

    The first step is to create a function for generating the co-occurrence matrix. Initially, we can implement a simple, non-optimized version. This is simply create an initial matrix with dimensions corresponding to the number of unique codes in the dataset and iterating through each patient to count occurrences. (code not displayed in the pdf file)

```{r}
#| echo: false
cooccur_old <- function(data, id, code, time, window = NA) {
  # Get unique ICD codes from the data to 
  unique_codes <- unique(data[[code]])
  code_count <- length(unique_codes)
  
  # Build a matrix filled with 0 as a start point for the counts
  cooccurrence_matrix <- matrix(0, nrow = code_count, ncol = code_count)
  rownames(cooccurrence_matrix) <- unique_codes
  colnames(cooccurrence_matrix) <- unique_codes
  
  # Iterate through each patient
  patients <- unique(data[[id]])
  for (patient in patients) {
    # Get data for the specific patient and sort by time column 
    patient_data <- data %>%
      filter(!!sym(id) == patient) %>%
      arrange(!!sym(time))
    
    # For patient who how only one row of record, there is no co-occurrence code within the patient
    # Skip patients with fewer than 2 observations
    if (nrow(patient_data) < 2) {
      next  # Skip this patient if there's only one or no observations
    }
    
    # To allow dynamic need of the user, allow function to have patient-specific window
    # If window is NA, set window to be the number of rows for this patient
    if (is.na(window)) {
      patient_window <- nrow(patient_data)
    } else {
      patient_window <- window
    }
    
    # For each ICD code in patient data, compare with others within the window
    # I am guessing that this for-loop can be vectorized. If not,
    # C++ would probably be a good fit here.
    for (i in 1:(nrow(patient_data) - 1)) {
      for (j in (i + 1):nrow(patient_data)) {
        if (patient_data[[time]][j] - patient_data[[time]][i] > patient_window) break

        code_i <- patient_data[[code]][i]
        code_j <- patient_data[[code]][j]
        
        # Update the co-occurrence matrix
        cooccurrence_matrix[code_i, code_j] <- cooccurrence_matrix[code_i, code_j] + 1
        cooccurrence_matrix[code_j, code_i] <- cooccurrence_matrix[code_j, code_i] + 1
      }
    }
  }
  
  return(cooccurrence_matrix)
}

```

Additionally, the code is initially optimized by data.table and vectorization (code not displayed in the pdf file)

```{r}
#| echo: false
library(data.table)

cooccur_optimized <- function(data, id, code, time, window = NA) {
  # Convert the input data into data.table for faster processing
  setDT(data)
  
  # Get unique ICD codes and map them to matrix indices
  unique_codes <- unique(data[[code]])
  code_count <- length(unique_codes)
  code_to_index <- setNames(seq_along(unique_codes), unique_codes)
  
  # Initialize an empty co-occurrence matrix
  cooccurrence_matrix <- matrix(0, nrow = code_count, ncol = code_count)
  rownames(cooccurrence_matrix) <- unique_codes
  colnames(cooccurrence_matrix) <- unique_codes
  
  # Process each patient
  patients <- unique(data[[id]])

  # This is another opportunity for optimization. You can do this
  # using parallel computing!
  for (patient in patients) {
    # Filter and sort data for the patient
    patient_data <- data[get(id) == patient, .(time_value = get(time), code_value = get(code))][order(time_value)]
    
    # Skip patients with fewer than 2 observations
    if (nrow(patient_data) < 2) next
    
    # Set window size
    patient_window <- if (is.na(window)) nrow(patient_data) else window
    
    # Loop over the patient data to compute co-occurrence within the window
    # Same here. You can see an example of vectorization similar to this
    # in this repo: https://github.com/UofUEpiBio/PHS7045-advanced-programming/blob/f0cd7a65d0370ed8d85854e9a52825dd365d2a4c/projects/03-life/life.r#L36-L49
    # This is fully vectorized entry-level comparison, which makes it
    # very fast. We can talk more in class, if you want.
    for (i in 1:(nrow(patient_data) - 1)) {
      for (j in (i + 1):nrow(patient_data)) {
        # Break if the time difference exceeds the window
        if (patient_data$time_value[j] - patient_data$time_value[i] > patient_window) break
        
        code_i <- patient_data$code_value[i]
        code_j <- patient_data$code_value[j]
        
        index_i <- code_to_index[[code_i]]
        index_j <- code_to_index[[code_j]]
        
        # Update co-occurrence counts
        cooccurrence_matrix[index_i, index_j] <- cooccurrence_matrix[index_i, index_j] + 1
        cooccurrence_matrix[index_j, index_i] <- cooccurrence_matrix[index_j, index_i] + 1
      }
    }
  }
  
  return(cooccurrence_matrix)
}


```

Beyond step one, other functions also have been written out as a manuscript (code not displayed in pdf file) and will be further optimized before the project finalized.

```{r}
#| echo: false
get_SG <- function(matrix){
  marg_word = coccur %>% group_by(code1) %>% summarise(marg=sum(count))
  marg_context = coccur %>% group_by(code2) %>% summarise(marg=sum(count))
  names(marg_word) = c("code","marg_count")
  names(marg_context) = c("code","marg_count")
  D = sum(as.numeric(coccur$count))
  return(list(marg_word=marg_word,marg_context=marg_context,D=D))
}
```

```{r}
#| echo: false
construct_pmi <- function(coccur,singletons,my.smooth=0.75){
  names(coccur) = c("code1","code2","joint_count")
  ind <- which(coccur$code1!=coccur$code2 &
                 coccur$code1%in%singletons$marg_word$code & 
                 coccur$code2%in%singletons$marg_context$code)
  coccur = coccur[ind,]
  
  coccur$joint_count = as.numeric(coccur$joint_count)
  singletons$marg_word$marg_count = as.numeric(singletons$marg_word$marg_count)
  singletons$marg_context$marg_count = as.numeric(singletons$marg_context$marg_count)
  
  pmi_df <- coccur %>%
    inner_join(singletons$marg_word,by=c("code1" = "code")) %>%
    dplyr::rename(W=marg_count) %>%
    inner_join(singletons$marg_context,by=c("code2" = "code")) %>%
    dplyr::rename(C=marg_count) %>%
    mutate(  PMI = joint_count/(W * ((C/singletons$D)^my.smooth))  ) %>%
    mutate(PMI=log(PMI)) 
  # %>%select(code1,code2,PMI)
  return(pmi_df)
}
```

```{r}
#| echo: false
construct_sppmi <- function(pmi,k=10) {
  sppmi_df <- pmi %>%
    mutate(SPPMI = pmax(PMI - log(k),0))%>%  
    select(code1,code2,SPPMI)
  
  # sppmi_df <- pmi %>%
  #   mutate(SPPMI = PMI) 
  
  all_words <- unique(c(sppmi_df$code1,sppmi_df$code2))
  word_2_index <- 1:length(all_words)
  names(word_2_index) <- all_words
  
  i <- as.numeric(word_2_index[as.character(sppmi_df$code1)])
  j <- as.numeric(word_2_index[as.character(sppmi_df$code2)])
  x <- as.numeric(sppmi_df$SPPMI)
  
  ## Remove 0s ##
  non_zero <- which(x != 0)
  i <- i[non_zero]
  j <- j[non_zero]
  x <- x[non_zero]
  if(max(i)<length(all_words)|max(j)<length(all_words)){
    i=c(i,length(all_words))
    j=c(j,length(all_words))
    x=c(x,0)
  }
  
  ism <- c(i,j)
  jsm <- c(j,i)
  xsm <- c(x,x)
  sppmi <- sparseMatrix(i=ism,j=jsm,x=xsm)
  rownames(sppmi) <- all_words
  colnames(sppmi) <- all_words
  return(sppmi)
}
```

-   The execution of the functions and performance comparison

    Since the diagnoses in the dataset are not time-stamped, it is impossible to determine the exact order of the diagnosis codes. Therefore, it is reasonable to treat all ICD codes within each patient as co-occurring to apply function as below:

    cooccur_optimized(data = data, id = "SUBJECT_ID", code = "ICD9_CODE", time = "SEQ_NUM", window = NA)

    The output of the function is the count for each code pair:

```{r}
#| echo:  false
#| warning: false
data <- fread("https://raw.githubusercontent.com/yidanzh0518/project/main/data/sample_data.csv")
# The execution of the functions
matrix_op <- cooccur_optimized(data = data, id = "SUBJECT_ID", code = "ICD9_CODE", time = "SEQ_NUM", window = NA)
head(matrix_op[1:8,1:8])
```

To evaluate performance, the `microbenchmark()` function was used to compare the execution time of both functions on the sample data.

```{r}
#| echo:  false
#| warning: false
# Benchmark on both functions to compare performance
microbenchmark(
  initial = cooccur_old(data = data, id = "SUBJECT_ID", code = "ICD9_CODE", time = "SEQ_NUM", window = NA),
  optimized = cooccur_optimized(data = data, id = "SUBJECT_ID", code = "ICD9_CODE", time = "SEQ_NUM", window = NA),
  times = 5  # Number of times to run each function for more accurate comparison
)

```

# Conclusion 

This project will be completed under the current progress, with further optimization aimed at reducing computational overhead. So far, we have incorporated data.table, vectorization, and timing techniques learned during the first half of the semester. Additional optimization approaches will also be explored as the project continue.
